# -*- coding: utf-8 -*-
"""pd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15IGTFNYlg15T6YDw7J3QPwSLh8Pi4uJ5
"""

# !pip install deepctr
# !pip install deepctr-torch

# !pip install --upgrade tensorflow
# from tensorflow.keras.layers import LSTM, Lambda, Layer, Dropout

import pandas as pd

df= pd.read_excel("/content/parkinsons_updrs.xlsx")

df



df.isnull().sum()

df.info()

df.corr()





import matplotlib.pyplot as plt
import seaborn as sns



plt.figure(figsize=(10,10))
sns.heatmap(df.corr(), annot=True, annot_kws={"size": 6})

import matplotlib.pyplot as plt

# Statistical summary of motor_UPDRS
motor_updrs_stats = df['motor_UPDRS'].describe()

# Plot the distribution of motor_UPDRS
plt.figure(figsize=(10, 6))
plt.hist(df['motor_UPDRS'], bins=30, color='blue', alpha=0.7, edgecolor='black')
plt.title('Distribution of motor_UPDRS', fontsize=14)
plt.xlabel('motor_UPDRS Score', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(True, alpha=0.5)
plt.show()

# Display the statistical summary
motor_updrs_stats

df['motor_UPDRS']

df["motor_UPDRS"].describe()

df["total_UPDRS"].info

for i in df.columns:
  plt.figure(figsize=(5,5))
  sns.histplot(df[i])

for i in df.columns:
  plt.figure(figsize=(5,5))
  sns.distplot(df[i])

A=["motor_UPDRS","HNR","total_UPDRS","sex","test_time"]

df1=df.drop(A, axis='columns')

df1

plt.figure(figsize=(10,10))
sns.heatmap(df1.corr(), annot=True, annot_kws={"size": 6})



# for column_name in df1.columns:
#   avg = df1[column_name].mean() # Use column_name instead of 'i'
#   std = df1[column_name].std() # Use column_name instead of 'i'
#   normalized = (df1[column_name] - avg) / std # Use column_name instead of 'column_name'
#   df1[column_name] = normalized # Use column_name instead of 'column_name'

df1

df.columns

# x= df1.drop("motor_UPDRS", axis = 'columns')

y=df["motor_UPDRS"]

y.value_counts()

# Define thresholds
threshhold1 = 27.596500
threshhold2 = 15.000000

# Apply lambda function to classify into 3 classes
df["pd_detect"] = df["motor_UPDRS"].apply(
    lambda x: 2 if x > threshhold1 else (1 if threshhold2 < x <= threshhold1 else 0)
)

y= df["pd_detect"]

y.value_counts()

y=df["pd_detect"]



from sklearn.model_selection import train_test_split

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(df1, y, test_size=0.3, random_state=42)

import seaborn as sns

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

_pred = knn.predict(X_test)

accu= accuracy_score(y_test, y_pred)
print(accu)

from sklearn.metrics import precision_score, recall_score, accuracy_score

# Example: Multiclass classification
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')  # Use 'macro', 'micro', or 'weighted'
recall = recall_score(y_test, y_pred, average='macro')        # Similarly adjust the average parameter

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Bar plot for accuracy, precision, and recall
metrics = ['Accuracy', 'Precision', 'Recall']
values = [accuracy, precision, recall]
plt.figure(figsize=(8, 6))
plt.bar(metrics, values, color=['blue', 'orange', 'green'])
plt.ylim(0, 1)
plt.title('Performance Metrics')
plt.ylabel('Score')
plt.xlabel('Metrics')
plt.show()

from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

k_values =[i for i in range(1,41)]
scores=[]

# scaler = StandardScaler()
# x= scaler.fit_transform(x)

for k in k_values:
  knn = KNeighborsClassifier(n_neighbors=k)
  score = cross_val_score(knn, df1, y, cv=5)
  scores.append(np.mean(score))

sns.lineplot(x = k_values, y = scores, marker = 'o')
plt.xlabel("K Values")
plt.ylabel("Accuracy Score")

import xgboost as xgb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

model = xgb.XGBClassifier()

#Training the model on the training data
model.fit(X_train, y_train)

#Making predictions on the test set
predictions = model.predict(X_test)

#Calculating accuracy
accuracy = accuracy_score(y_test, predictions)

target_names = ['0', '1', '2']  # Replace with meaningful names

print("Accuracy:", accuracy)
print("\nClassification Report:")
print(classification_report(y_test, predictions, target_names=target_names))

!pip install LibRecommender

import numpy as np
import pandas as pd
from libreco.data import split_by_ratio_chrono, DatasetFeat
from libreco.algorithms import DeepFM

y

df.columns

df

import pandas as pd
from deepctr_torch.models import DeepFM
from deepctr_torch.inputs import DenseFeat, get_feature_names
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score , accuracy_score
# from deepctr.models import DeepFM
# from deepctr.feature_column import DenseFeat, get_feature_names
import tensorflow as tf

A=["HNR","total_UPDRS","sex","test_time"]
data= df.drop(A, axis= 'columns')

# Define target and features
target = 'motor_UPDRS'  # Replace with 'total_UPDRS' if desired
features = [col for col in data.columns if col != target]

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Normalize numerical features
scaler = StandardScaler()
train_data[features] = scaler.fit_transform(train_data[features])
test_data[features] = scaler.transform(test_data[features])

# Prepare feature columns for DeepFM
feature_columns = [DenseFeat(feat, 1) for feat in features]
feature_names = get_feature_names(feature_columns)

# Create input data for the model
train_model_input = {name: train_data[name].values for name in feature_names}
test_model_input = {name: test_data[name].values for name in feature_names}

# Define the model
model = DeepFM(linear_feature_columns=feature_columns,
               dnn_feature_columns=feature_columns,
               task='regression')

from tensorflow.keras.optimizers import Adam

# Define the learning rate
learning_rate = 0.01  # You can adjust this value

# Create the optimizer with the specified learning rate
optimizer = Adam(learning_rate=learning_rate)
# Compile the model
from tensorflow.keras.metrics import RootMeanSquaredError

model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])

# Train the model
history = model.fit(train_model_input, train_data[target].values,
                    batch_size=8, epochs=25,
                    validation_split=0.1)

# Evaluate the model
train_pred = model.predict(train_model_input, batch_size=16)
train_mse = mean_squared_error(train_data[target].values, train_pred)
train_mae = mean_absolute_error(train_data[target].values, train_pred)
train_r2 = r2_score(train_data[target].values, train_pred)
test_pred = model.predict(test_model_input, batch_size=16)
test_mse = mean_squared_error(test_data[target].values, test_pred)
test_r2 = r2_score(test_data[target].values, test_pred)
trainr2 = r2_score(train_data[target].values, model.predict(train_model_input, batch_size=32))
print(f"Test Mean Squared Error: {test_mse}")
print(f"Test R^2 Score: {test_r2}")
print(f"Train R^2 Score: {trainr2}")

# Save predictions
test_data['Predicted'] = test_pred
test_data[['motor_UPDRS', 'Predicted']].to_csv('predictions.csv', index=False)
print("Predictions saved to predictions.csv")

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# # Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42)

# train_data= lgb.Dataset(X_train,label =y_train)
# test_data = lgb.Dataset(X_test,label= y_test, reference=train_data)

import re
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming X_train, X_test, y_train, y_test are pandas DataFrames/Series
X_train = X_train.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))
X_test = X_test.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))

# Create LightGBM Datasets
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test)

# Define parameters
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 15,
    'learning_rate': 0.01,
    'feature_fraction': 0.8
}

# Train the model
model = lgb.train(
    params,
    train_set=train_data,
    valid_sets=[test_data],
    # early_stopping_rounds=50,
    num_boost_round=150
)

# Evaluate the model
y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)
y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)

mae_ = mean_absolute_error(y_test, y_pred_test)
mse = mean_squared_error(y_test, y_pred_test)
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"R²_train: {r2_train}")
print(f"R²_test: {r2_test}")

# from sklearn.model_selection import GridSearchCV

# param_grid = {
#     'num_leaves': [7, 15, 31],
#     'learning_rate': [0.001, 0.01, 0.1],
#     'feature_fraction': [0.6, 0.7, 0.8]
# }

# grid_search = GridSearchCV(estimator=lgb.LGBMRegressor(),
#                            param_grid=param_grid,
#                            scoring='neg_mean_squared_error',
#                            cv=5,
#                            verbose=2)

# grid_search.fit(X_train, y_train)
# best_params = grid_search.best_params_
# print(best_params)

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression

# Step 1: Train DeepFM and LightGBM (assuming already done)
# Predictions from DeepFM
if isinstance(train_model_input, dict):
    X_train = pd.DataFrame(X_train)
if isinstance(test_model_input, dict):
    X_test = pd.DataFrame(X_test)
train_pred_deepfm = model.predict(X_train, batch_size=32)
test_pred_deepfm = model.predict(X_test, batch_size=32)

# Predictions from LightGBM
train_pred_lgb = model.predict(X_train, num_iteration=model.best_iteration)
test_pred_lgb = model.predict(X_test, num_iteration=model.best_iteration)

# Step 2: Create meta-model input
# Combine predictions from base models
train_meta_features = np.column_stack((train_pred_deepfm, train_pred_lgb))
test_meta_features = np.column_stack((test_pred_deepfm, test_pred_lgb))

# Step 3: Define and train the meta-model
meta_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.01, max_depth=3)
meta_model.fit(train_meta_features, y_train)

# Step 4: Make final predictions with the meta-model
final_train_pred = meta_model.predict(train_meta_features)
final_test_pred = meta_model.predict(test_meta_features)

# Step 5: Evaluate the hybrid model
train_mae = mean_absolute_error(y_train, final_train_pred)
# test_mae = mean_absolute_error(y_test, final_test_pred)
final_train_mse = mean_squared_error(y_train, final_train_pred)
final_test_mse = mean_squared_error(y_test, final_test_pred)
final_train_r2 = r2_score(y_train, final_train_pred)
final_test_r2 = r2_score(y_test, final_test_pred)

print(f"Final Train MSE: {final_train_mse}")
print(f"Final Test MSE: {final_test_mse}")
print(f"Final Train R^2: {final_train_r2}")
print(f"Final Test R^2: {final_test_r2}")

# Save final predictions
final_predictions = pd.DataFrame({
    'Actual': y_test,
    'Predicted': final_test_pred
})
final_predictions.to_csv('hybrid_model_predictions.csv', index=False)
print("Hybrid model predictions saved to hybrid_model_predictions.csv")

# Values extracted from the code:
train_r2_model1 = trainr2  # Assuming from example train_r2
test_r2_model1 = test_r2 # Assuming from example test_r2

train_r2_model2 = r2_train  # From r2_train for Model 2
test_r2_model2 = r2_test   # From r2_test for Model 2

train_r2_hybrid = final_train_r2  # From final_train_r2 for Hybrid Model
test_r2_hybrid = final_test_r2  # From final_test_r2 for Hybrid Model

# Updated values
train_r2_scores = [train_r2_model1, train_r2_model2, train_r2_hybrid]
test_r2_scores = [test_r2_model1, test_r2_model2, test_r2_hybrid]

# Re-plotting the R² comparison with updated values
fig, ax = plt.subplots(figsize=(8, 5))
models = ['DeepFM', 'LightGBM', 'Hybrid(XGboost)']
x = np.arange(len(models))
width = 0.4

# Only plotting R² values
ax.bar(x - width / 2, train_r2_scores, width, label='Train R²', alpha=0.7)
ax.bar(x + width / 2, test_r2_scores, width, label='Test R²', alpha=0.7)

# Labels, title, and legend
ax.set_xlabel('Models', fontsize=12)
ax.set_ylabel('R² Scores', fontsize=12)
ax.set_title('R² Score Comparison Across Models', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(models, fontsize=10)
ax.legend(fontsize=10)


plt.tight_layout()
plt.show()

# Correcting the train loss plot

# Sample loss values for demonstration purposes (replace with actual values if available)
train_loss_model1 = train_mae  # Replace with actual train loss for Model 1
# test_loss_model1 = 0.20   # Replace with actual test loss for Model 1

train_loss_model2 =mae_  # Replace with actual train loss for Model 2
# test_loss_model2 = 0.17   # Replace with actual test loss for Model 2

train_loss_hybrid = train_mae # Replace with actual train loss for Hybrid Model
# test_loss_hybrid = 0.12   # Replace with actual test loss for Hybrid Model

# Updated loss values
train_loss_scores = [train_loss_model1, train_loss_model2, train_loss_hybrid]

# Re-plotting the train loss
fig, ax = plt.subplots(figsize=(8, 8))
models = ['DeepFM', 'LightGBM', 'Hybrid(XGboost)']
x = np.arange(len(models))
width = 0.4
# Plotting Train Loss
ax.bar(x, train_loss_scores, width, label='Train Loss', alpha=0.7, color='skyblue')

# Labels, title, and legend
ax.set_xlabel('Models', fontsize=12)
ax.set_ylabel('Loss (MAE)', fontsize=12)
ax.set_title('Train Loss (MAE) Comparison Across Models', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(models, fontsize=10)
ax.legend(fontsize=10)


plt.tight_layout()
plt.show()

